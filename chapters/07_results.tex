\chapter{Ergebnisse}
\label{ch:results}

In diesem Kapitel werden die Ergebnisse der in \autoref{sec:experimente} beschriebenen Experimente vorgestellt.
Die Evaluation schließt die Ergebnisse aller vier Phasen, häufige Fehler in der Verifikation, sowie eine grobe Kategorisierung der Klassen ein.

\section{Benchmark}
\label{sec:benchmark}

In diesem Abschnitt werden die Rechenergebnisse der verschiedenen Modelle miteinander verglichen.  %, sowie verschiedene Verfahren Modelle mit unterschiedlich langen Input-Clips zu vergleichen.
Als Baseline-Modelle wurden R2+1D-34, ir-CSN-152 und zwei Varianten von SlowFast-50 getestet und mit $\gls{tld:Theta}_\text{train} = 200$ für 10 Epochen trainiert.
Beide SlowFast-Varianten wurden zunächst ohne Hinzunahme von Non-Local-Blöcken trainiert.
In \autoref{tab:phase1} sind die Ergebnisse zu sehen, wobei alle Hyperparameter aus den ursprünglichen Publikationen übernommen wurden.
Die Metriken basieren auf einem einheitlichen Testset mit $\Delta_\text{test} = 3$.
Die Clip-Dauer wird dabei stets durch zusätzliche Frames erreicht (\zB 36 statt 32 Frames bei SlowFast).

Überraschend ist, dass die SlowFast-Variante mit $\alpha = 8$ (SlowFast-50-4x16) bessere Ergebnisse erzielt als die Variante mit $\alpha = 4$ (SlowFast-50-8x8).
Im ersteren erhält der Slow-Pathway schließlich nur halb so viele Frames, wie im letzteren und kann sich damit noch mehr auf räumliche Features fokussieren.
Da die Ergebnisse für SlowFast-50-4x16 durchweg besser ausfallen, wird diese Variante im weiteren Verlauf als SlowFast-Baseline-Modell bezeichnet.

\begin{figure}
    \centering
    \small
    \csvreader[no head,tabular=|l|r||r|r|r|r|,
    table head=\hline,late after line=\\\hline]{tbl/exp_phase_1.csv}
    {1=\model,2=\aurocval,3=\baval,4=\fbetaval,5=\lr,6=\bs,7=\ba,8=\rec,9=\prec,10=\auroc}
    {\model & \lr & \ba & \prec & \rec & \auroc}
    \caption{Ergebnisse aus Benchmark}
    \label{tab:phase1}
\end{figure}

\subsection{Vergleich der Modelle}
\label{subsec:vergleich-der-modelle}

Vergleicht man die Ergebnisse, fällt auf, dass ir-CSN die besten und R2+1D die schwächsten Metriken vorweist.
Damit einher geht allerdings auch ein größerer Fußabdruck:
während bei R2+1D (\bzw SlowFast) eine maximale Batch-Größe von 24 (\bzw 21) möglich war, erlaubt ir-CSN nur eine Batch-Größe von maximal 5 bei gleicher Speicherauslastung.
Dennoch wird für den weiteren Verlauf ir-CSN als einziges Baseline-Modell weitergeführt.
Die weitere Optimierung der anderen Modelle wird aus Zeitgründen nicht weiter verfolgt.

\subsection{Verifikation}
\label{sec:verifikation}

Im nächsten Schritt werden die Fehler der Experimente genauer analysiert.
Grundlage sind die drei Baseline-Modelle aus der ersten Phase.
Im Zuge der Analyse werden innerhalb einer statistischen Analyse Auffälligkeiten im Fehlerreport analysiert.
Zusätzlich werden jeweils 50 der Samples mit den größten Fehlerwerten innerhalb eines Experiments verifiziert.

Bei der statistischen Analyse werden die Samples nach Halbzeitvideo gruppiert und der Mittelwert der Fehler wird berechnet.
Dadurch konnten insgesamt vier fehlerhafte Videos erkannt werden, da diese auffällig hohe Mittelwerte hatten (mit einem z-Score über 3).
Zudem wurde für zwei weitere Videos, die nach dem gleichen Kriterium aufgefallen sind, das manuelle Aligning nachjustiert.

Bei der Verifikation einzelner Samples wurden pro Modell 50 Samples (im Verhältnis 3:1:1 je Datenset) mit den höchsten Fehlerwerten manuell verifiziert.
Dies entspricht 150 Samples, bei denen insgesamt 267 Transaktionen mit Korrekturvorschlägen gespeichert wurden.
Die Transaktionen bestehen zu 72.6 \% aus Zeitfensteranpassungen.
22.8 \% sind Löschungen von nicht sichtbaren Aktionen und 4.5 \% sind hinzugefügte Aktionen.
Die häufigsten Klassen für Zeitfensteranpassungen sind \code{save} (32), \code{footShot} (30), \code{cross} (14) und \code{card} (10).
Besonders auffällig ist, dass \zB \code{save}- oder \code{card}-Aktionen in den Stammdaten, teils mehrere Sekunden, zu früh auftreten.
Nach Durchsicht der Daten werden die 267 Transaktionen in die Datenbank übernommen.
Alle weiteren Experimente beziehen sich somit auf die korrigierte Datenbank.

Eine weitere Optimierung des Baseline-Modells anhand der verifizierten Datenbank, führt zu einer zusätzlichen Steigerung der Balanced Accuracy von XXX Prozent (siehe \autoref{tab:phase1}).

\section{Hyperparameter-Optimierung}
\label{sec:hyperparameter-optimierung}

Spielaktionen können unter Berufung auf \gls{sbod} zwischen bis zu 6 Sekunden dauern.
Die vortrainierten Modelle bilden in ihrer ursprünglichen Form jedoch maximal 2.56 Sekunden ab.
Im Zuge der zweiten Experimentphase werden verschiedene Hyperparameter mithilfe einer gierigen Grid-Suche getestet.

Alle Modelle nutzen eine dynamisches Avg-\pool-Layer zwischen dem letzten \conv- und dem ersten \fc-Layer, das die Verarbeitung von Tensoren variabler Länge $T$ zulässt.
Um eine geeignete Methode zu finden, wird der Einfluss von Avg-Pooling und von Subsampling auf den jeweils gleichen Daten pro Modell verglichen.

Die Grid-Suche betrachtet Punkte für verschiedene Werte von $T$ und $\tau$.
Beginnend mit dem Baseline-Modell aus der ersten Phase, werden iterativ mehrere Parameterkombination ausprobiert, sodass sich die Clip-Dauer um einen festen Faktor erhöht.
Die Konfigurationen werden verglichen und die erfolgreichere wird wieder zum Startpunkt für die nächste Iteration.
Die Suche endet sobald, es keine Verbesserungen mehr stattfindet oder das Training aufgrund von Hardwarebeschränkungen nicht mehr möglich ist.
Mit den genannten Einschränkung ist die Suche deutlich schneller, aber auch unvollständiger als eine gewöhnliche Grid-Suche.

\begin{figure}
    \centering
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth, keepaspectratio, interpolate]{img/07_grid_r2plus1d.eps}
    \end{subfigure}%
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth, keepaspectratio, interpolate]{img/07_grid_slowfast.eps}
    \end{subfigure}
    \begin{subfigure}{.33\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth, keepaspectratio, interpolate]{img/07_grid_csn.eps}
    \end{subfigure}
    \caption{Grid-Suche für optimale Hyperparameter}
    \label{fig:exp-hparams}
\end{figure}

\autoref{fig:exp-hparams} zeigt die Ergebnisse, indem pro Modell ausgehend vom Baseline-Modell die Clip-Dauer erhöht wird.
Die Iterationen sind hier durch Linien dargestellt, die je Modelle einer festen Clip-Dauer trainieren.
Aufgrund der Vielzahl zusätzlicher Experimente wurden die Laufzeit bei diesen Experimenten durch $\gls{tld:Theta}_\text{train} = 100$ und auf 3 Epochen begrenzt.

Als Startpunkt wird das Baseline-Modell der vorherigen Phase gewählt.
Da das Baseline-Modell nach der ersten Phase bereits neue Feature gelernt hat, werden zu Beginn neue Lernraten gesucht, die für alle Trainings der Grid-Suche gelten.
Das jeweils beste Modell wird nach Abschluss der Suche erneut für 10 Epochen mit $\gls{tld:Theta}_\text{train} = 200$ trainiert (analog zu Phase 1).
\autoref{tab:phase2} vergleicht die Ergebnisse der ersten zwei Phasen.

\begin{figure}
    \centering
    \csvreader[no head,tabular=|l|r|r|r||r|r|r|,
    table head=\hline,late after line=\\\hline]{tbl/exp_phase_2.csv}
    {1=\model,2=\s,3=\t,4=\sr,5=\d,6=\result,7=\ihatelatex,8=\reallyshittylatex}
    {\model & \t & \sr & \d & \result & \ihatelatex & \reallyshittylatex}
    \caption{Ergebnisse aus Phase 2}
    \label{tab:phase2}
\end{figure}

Die Ergebnisse zeigen, dass ir-CSN das Problem am besten löst und in allen Metriken die besten Ergebnisse hat.
Daher wird der Fokus im weiteren Verlauf ausschließlich auf dieses eine Modell eingeschränkt.

\section{Kategorisierung der Aktionsklassen}
\label{sec:kategorisierung-der-aktionsklassen}

\begin{tcolorbox}[title=Todo]
    \begin{itemize}
        \item Abweichung pro Metrik erheben: Klasse zu macro -> Durchschnitt
        \item Idee: pca und clustering in vier gruppen
        \item Vergleich der Evaluation auf subsets (SOCC-HAR-8, -16, -24, -32): Jeweils ROC Curve
        \item Optional: SOCC-HAR-NET, SOCC-HAR-DB Klassen
    \end{itemize}
\end{tcolorbox}

\begin{figure}
    \centering
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth, keepaspectratio, interpolate]{img/07_balanced_accuracy_by_class_test_202012-2218-2841.eps}
    \end{subfigure}%
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth, keepaspectratio, interpolate]{img/07_f1_by_class_test_202012-2218-2841.eps}
    \end{subfigure}
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth, keepaspectratio, interpolate]{img/07_auroc_by_class_test_202012-2218-2842.eps}
    \end{subfigure}
    \begin{subfigure}{.24\textwidth}
        \centering
        \includegraphics[width=0.99\textwidth, keepaspectratio, interpolate]{img/07_PCA_by_class_test_202012-2218-2843.eps}
    \end{subfigure}
    \caption{Metriken pro Klasse}
    \label{fig:class-metrics}
\end{figure}

Die bisherigen Rechenergebnissen wurden jeweils die durchschnittlichen Metriken (macro) über alle Klassen betrachtet.
In diesem Abschnitt wird das Top-Modell hingegen an jeder Klasse einzeln evaluiert.
Dazu werden die bereits verwendeten Metriken pro Klasse erhoben.
Wie \autoref{fig:class-metrics} zu entnehmen ist, gibt es starke Abweichungen innerhalb der verschiedenen Klassen.
Während Aktionen wie \code{corner}, \code{footShot} oder \code{throwIn} durchgehend hohe Metriken vorweisen, können andere Klassen wie \code{offside} oder \code{handball} praktisch gar nicht klassifizieren werden.
Zwischen diesen Extrema ist es schwierig eine klare Rangordnung von leicht zu schwierig herzustellen.
Das wird insbesondere deutlich wenn man die Metriken Precision und Recall in \autoref{fig:precision-recall} betrachtet.

\begin{figure}
    \centering
    \includegraphics[width=0.99\textwidth, keepaspectratio, interpolate]{img/07_precision_recall_by_class.eps}
    \caption{Vergleich von Precision und Recall pro Klasse}
    \label{fig:precision-recall}
\end{figure}

Dabei fällt auch auf, dass vor die Klassen im unteren Bereich mit einer niedrigen Precision viele False Positives verursachen, obwohl sie zum Teil einen hohen Recall haben.
Auffällig ist auch, dass Aktionen, die sich im laufenden Spiel ereignen (\code{foul}, \code{cross}, \code{footShot}, \code{save}) einen besonders hohen Recall haben, während Aktionen die oft mit Spielunterbrechungen und Nahaufnahmen einhergehen (\code{substitution}, \code{goal}, \code{injury}) einen sehr niedrigen Recall aufweisen.

Man sieht also, dass keine pauschale Aussage über die Schwierigkeit der Aktionsklassen gemacht werden kann.
Dennoch lässt sich anhand der erfassten Metriken eine grobe Ordnung erstellen, auch wenn sie allgemein nicht repräsentativ ist.
Hierzu werden alle Metriken pro Klasse zu einer zweidimensionalen Matrix $M \in (N \times 5)$ gesammelt und im Zuge einer \gls{pca} auf einen eindimensionalen Vektor projiziert.
Die erste Komponente der \gls{pca} $c_0 \in (5 \times 1)$ deckt mit 80 \% hinreichend viel Varianz und projiziert die Metriken auf einen Vektor:

\begin{equation}
    \label{eq:pca}
    \text{pca}_0 = M \cdot c_0
\end{equation}

Die resultierende Rangordnung ist ebenfalls in \autoref{fig:class-metrics} (rechts) abgebildet.

\subsection{Evaluation auf Teilmengen}
\label{subsec:evaluation-auf-teilmengen}

Abschließend soll der Einfluss schwächerer Klassen auf das Gesamtergebnis gemessen werden.
Dazu werden die Aktionsklassen absteigend nach der PCA-basierten Rangordnung sortiert und in vier Teilmengen gegliedert, die sich jeweils aus den oberen 8, 16, 24 und 32 Klassen zusammensetzen.
In \autoref{tab:eval_subset} sind die Ergebnisse (macro) auf Berechnungsgrundlage der in der jeweiligen Teilmenge enthaltenen Klassen aufgeführt.
Zusätzlich wurden zwei weitere Untermengen mit den gleichen Aktionsklassen aus den Datensets SoccerNet und SoccerDB abgeleitet.
Die Ergebnisse sind keinesfalls direkt vergleichbar mit den publizierten Datensets, da die Datengrundlage eine andere ist.
Es soll dem Leser jedoch eine grobe Einordnung ermöglichen.

\begin{figure}
    \centering
    \csvreader[no head,tabular=|l||r|r|r|,
    table head=\hline,late after line=\\\hline]{tbl/eval_subsets.csv}
    {1=\dataset,2=\ba,3=\fbeta,4=\auroc}
    {\dataset & \ba & \fbeta & \auroc}
    \caption{Evaluation auf Teilmengen von Aktionsklassen}
    \label{tab:eval_subset}
\end{figure}

\section{Fine-Tuning}

\begin{tcolorbox}[title=WIP]
    \begin{itemize}
        \item Tabelle: Label Smooting, res,Anzahl Epochen, Weight Decay
        \item Mit mehr Background trainieren um Precision zu erhöhen!
        \item schon hier beschränken auf 24 Klassen?
        \item Threshold auf Validierungsset bestimmen und an Testset testen
    \end{itemize}
\end{tcolorbox}

In der vierten Phase wird das neue Baseline-Modell aus Phase 3 mit zusätzlichen Techniken weiter optimiert.
Dazu zählt das Training mit einer erhöhten Auflösung $S$, der Einsatz von Label Smoothing \cite{Szegedy15} und die Justierung weitere Hyperparameter wie Weight Decay.
\autoref{tab:exp4} zeigt die Ergebnisse.

\begin{figure}
    \centering
    \csvreader[no head,tabular=|l|r||r|r|r||r|r|r|,
    table head=\hline,late after line=\\\hline]{tbl/exp_phase_4.csv}
    {1=\model,2=\wdecay,3=\s,4=\thet,5=\auroc,6=\ba,7=\fbeta,8=\epo}
    {\model & \s & \epo & \thet & \wdecay & \ba & \fbeta & \auroc}
    \caption{Ergebnisse aus Phase 4}
    \label{tab:exp4}
\end{figure}